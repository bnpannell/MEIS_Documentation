% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{California Military Economic Impact Study Process Guide}
\author{Britnee Pannell \& Sumeet Bedi}
\date{2022-01-24}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={California Military Economic Impact Study Process Guide},
  pdfauthor={Britnee Pannell \& Sumeet Bedi},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\nocite{*}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

The California Military Economic Impact Study report series began in 2018, when the California Governor's Office of Planning \& Research (OPR) and the Governor's Military Council (GMC) requested the California Research Bureau (CRB, a unit of the California State Library) to conduct this study. This report provides detailed statewide and localized economic impacts of federal national security activity in the state of California. The federal agencies identified as related to national security includes the Departments of Defense (DoD), Homeland Security (DHS), Veterans Affairs (VA), and specified sub-agencies of the Department of Energy (DOE). The type of economic activity detailed includes spending (contracts, grants, veterans' benefits, and SmartPay charge card) and employment data (civilian and military). The first report, published in August 2018, utilized federal fiscal year 2016 data, while the second report, published in December 2019, used fiscal year 2018 data.

Following these 2 reports, OPR and CRB secured grant funding from DoD to support two full-time equivalent (FTE) positions to develop supplemental reports that help localize the economic impacts detailed in the statewide report. In December 2020, CRB produced the 2020 California Military Economic Impact Study, and followed this third edition of the statewide report with two first-time supplements that discuss the economic impacts in every county and congressional district in California. In December 2021, CRB completed the fourth version of the statewide report as well as the second edition of the county and congressional district supplements. After completion of the second round of supplements, CRB was tasked with producing this process guide document.

This process guide and supporting documentation were developed in order to allow other states to replicate the methodology of this study for their respective geography of interest. Additionally, this documentation serves to provide the rationale behind how the data was gathered, wrangled, and analyzed in order to justify the conclusions in our reports.

\hypertarget{software-requirements}{%
\chapter{Software Requirements}\label{software-requirements}}

To recreate or replicate this study, specialized software is needed to obtain and process federal data. Fortunately, all the software that was used in this process is free and available online. It is best practice to use the most updated software version.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The R coding language from \href{https://cloud.r-project.org/}{cloud.r-project.org}. This language is used to obtain and process data.
\item
  RStudio Desktop from \href{https://rstudio.com/products/rstudio/download/\#download}{rstudio.com}. RStudio is the integrated development environment (IDE) used to run R scripts and develop code.
\item
  Git from \href{https://git-scm.com/downloads}{git-scm.com}. A comprehensive guide to installing Git is available at \href{https://happygitwithr.com/install-git.html}{happygitwithr.com}. Git allows version control of edits across a multi-person team of researchers.
\item
  If a GitHub account is needed, one can sign up and register at \url{https://github.com/}. Individual free plans are available, as well as \href{https://help.github.com/en/articles/applying-for-an-educator-or-researcher-discount}{free upgrades} for qualifying academic purposes. GitHub is used to develop and host this project.
\end{enumerate}

\hypertarget{download-necessary-r-libraries}{%
\section{Download Necessary R Libraries}\label{download-necessary-r-libraries}}

To make sure the code can be run, please ensure that the following R library packages are installed. If you are using RStudio, packages can be installed from CRAN using the ``install.packages(\emph{{[}package\_name{]}})'' command in the terminal.

\begin{itemize}
\tightlist
\item
  \textbf{dplyr:} Used for manipulating dataframes. \citep{dplyr1}
\item
  \textbf{httr:} Used for working with URLs and HTTP. \citep{httr2}
\item
  \textbf{jsonlite:} Used for interacting with JSON data and web APIs. \citep{json3}
\item
  \textbf{openxlsx:} Used for reading, writing and editing xlsx files. \citep{openxlsx4}
\item
  \textbf{readxl:} Used for legacy excel .xls files. \citep{readxl5}
\item
  \textbf{tidyverse:} A collection of R packages optimized for data science. \citep{tidyverse6}
\end{itemize}

The next section details data necessary to complete this project as well as how to obtain it.

\hypertarget{data-requirements}{%
\chapter{Data Requirements}\label{data-requirements}}

This section details how to obtain all the necessary data for this project, which we broke out into 3 types: 1) employment, 2) USAspending, and 3) all other additional data. \emph{There is no guarantee that the data sources described below will exist in this form indefinitely, so care will be taken to keep this document as up to date as possible.}

\hypertarget{employment-data}{%
\section{Employment Data}\label{employment-data}}

This report focused on two types of employment data: civilian and military. These employment data types were found on two separate websites:

\textbf{Civilian employment} was obtained from the \href{https://www.fedscope.opm.gov/}{Office of Personnel Management's (OPM) FedScope website}, which provides federal workforce data. From this link, one can click on ``Employment'' under the ``Status Data'' bullet point to access quarterly employment data cubes. Given that the report focuses on federal fiscal years, the employment data of interest is of Quarter 3 of a given year (i.e.~September). When one clicks on a September cube of their desired year, one can filter the data by agency type (``Cabinet Level'') and location (``United States'' and a state of one's choice) in order to get the civilian employment numbers for the Departments of Defense (which is an aggregate of the Air Force, Army, Defense, and Navy cabinet agencies), Homeland Security, Veterans Affairs, and Energy. This data can be exported out as a PDF, CSV, or Excel file.

\textbf{Military employment} was obtained from the \href{https://dwp.dmdc.osd.mil/dwp/app/main}{Defense Manpower Data Center (DMDC) website}, which serves under the Office of the Secretary of Defense. From this link, one can hover over to DoD Data/Reports and select ``Statistics \& Reports''. On this page, scroll to the ``DoD Personnel, Workforce Reports \& Publications'' text, and click on its hyperlink. \href{https://dwp.dmdc.osd.mil/dwp/app/DoD-data-reports/workforce-reports}{This opens up a new window to access the military employment data}. Similar to civilian employment, the data of interest is from September of a given year. One can scroll to the section ``Military and Civilian Personnel by Service/Agency by State/Country (Updated Quarterly)'', and download the Excel file for September of their desired year.

\hypertarget{usaspending-data}{%
\section{USASpending Data}\label{usaspending-data}}

This report utilized \href{https://www.usaspending.gov/}{USAspending.gov} for a large majority of the direct spending done by federal national security agencies in California. The spending types obtained from this site include contracts and grants for DoD, DHS, VA and DOE, as well as direct payments from VA (i.e.~veterans' benefits). From this link, one can hover over to ``Download'' and click on ``Custom Award Data''. \href{https://www.usaspending.gov/download_center/custom_award_data}{On this webpage}, one can select the appropriate spending award types (contracts, grants, and/or direct payments), awarding agency (DoD, DHS, VA, DOE), recipient location (United States for ``Country'', and a specific state should you want to filter the data further), action date range (a fiscal year ``FY 20XX''), and file format. This download process must be done once per agency.

For obtaining USAspending data, we have developed code that automates this entire process and easily captures all USAspending data in one instance. Please refer to our methods section of this process guide for more details.

\hypertarget{additional-data}{%
\section{Additional Data}\label{additional-data}}

\hypertarget{smartpay-data-via-foia}{%
\subsection{SmartPay Data via FOIA}\label{smartpay-data-via-foia}}

The remaining portion of direct spending detailed in these reports was SmartPay, a charge card program for federal employees. In order to obtain this data, Freedom of Information Act requests (commonly known as FOIAs) were filed to the federal national security agencies.

For more information about SmartPay, please visit the \href{https://www.gsa.gov/travel/plan-book/gsa-smartpay}{General Services Administration's (GSA) SmartPay website}.

\hypertarget{data-obtained-online}{%
\subsection{Data Obtained Online}\label{data-obtained-online}}

A variety of data sources needed to be obtained online in order to assist with the data processing for this report. This includes:

\begin{itemize}
\item
  \href{https://www.gsa.gov/policy-regulations/policy/real-property-policy/asset-management/federal-real-property-profile-frpp/federal-real-property-public-data-set}{Federal Real Public Property data} from the GSA. At this link, the GSA provides an Excel file that breaks down how much federal property is spread out across the United States. This data is used to help localize the number of statewide DHS and VA civilian employees across counties and districts.
\item
  \href{https://data.census.gov/cedsci/}{American Community Survey data} from the Census Bureau. At this link, type into the search bar at the center of the webpage ``DP03''. This returns a table of ``Selected economic characteristics''. From this table, one can filter to the appropriate geography (state, county, congressional district, etc.), year, and such to download into an Excel file. This data is used to help localize the number of statewide military personnel across counties and districts.
\item
  \href{https://support.implan.com/hc/en-us/articles/360034896614-546-Industries-Conversions-Bridges-Construction-2018-Data}{NAICS to IMPLAN crosswalk} from IMPLAN. At this link, IMPLAN provides a variety of crosswalks that one can use to relate data from other source to IMPLAN's 546 industries. On this page, one can go to the second heading titled ``2017 NAICS to IMPLAN 546 Industries'' and download the Excel file. This crosswalk is utilized to help relate the USAspending contracts data to IMPLAN sectors in order to run that spending data through the IMPLAN software.
\end{itemize}

\hypertarget{data-provided-rawself-made}{%
\subsection{Data Provided Raw/Self-Made}\label{data-provided-rawself-made}}

The repository contains some raw data files that we have prepared for this project. Specific instructions on how to access this repository and utilize these raw data sources are explained in the next section.

\hypertarget{setting-up-the-repository}{%
\chapter{Setting Up the Repository}\label{setting-up-the-repository}}

Our code repository is necessary to repeat the California study. It may also be used as a frame work to run additional studies outside of the initial region. These instructions assume you will be using Github to obtain and manage a copy of the repository.

\hypertarget{obtain-repository}{%
\section{Obtain Repository}\label{obtain-repository}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using Github, navigate to the repository ({[}LINK TO REPO FOREVER HOME HERE{]}) and fork the repository to the user's account.
\item
  Clone the repository to a new RStudio Project.
\item
  Open the `README.md' file ({[}LINK{]})\\
\item
  Fill out the parameters file with required specifications. The `README.md' file contains general instructions, section 6.1 of the Methods section ({[}add link to section here{]}) has more detailed explanations.
\end{enumerate}

\hypertarget{modify-data-files}{%
\section{Modify Data Files}\label{modify-data-files}}

Section used for instructing how to set up data repositories for custom user made data? Link back to this section from section 6

\hypertarget{data-dictionary-for-repository}{%
\chapter{Data Dictionary for Repository}\label{data-dictionary-for-repository}}

This section details each file contained in the code repository and a summary of what it is used for in the project. For a more detailed explanation, please see the ``Methods'' section. {[}add link later{]}

\hypertarget{meis_methodology-folder-change-name-later}{%
\section{MEIS\_Methodology Folder {[}CHANGE NAME LATER{]}}\label{meis_methodology-folder-change-name-later}}

This is the main folder, containing all data in the repository.

\begin{itemize}
\tightlist
\item
  \textbf{.gitignore:} Lists files that will be ignored when using Git to synchronize updates to repository.
\item
  \textbf{.Rhistory:} Contains the history of commands given by a user in an R Session
\item
  \textbf{LICENSE:} Contains licensing information for the repository
\item
  \textbf{MEIS\_Methodology.Rproj:} the R project package used to open the repository in an IDE such as RStudio.
\item
  \textbf{README.md:} The first file you should read in any repository. Contains basic installation information, information on the repository contents and instructions on how to use the repository.
\item
  \textbf{parameters.R:} Contains all variables that may require user editing in order for code to run properly. Use of this file is documented in the README.md file.
\item
  \textbf{run\_analysis\_master.R:} The newest version of the code to process the Federal data. Currently this file is in development for the upcoming 2022 version of this project.
\item
  \textbf{DEPRECATED\_run\_analysis\_master.R:} This file contains the script used for the 2021 project. This process guide pertains mostly to the code running from this master script.
\end{itemize}

\hypertarget{data-folder}{%
\section{data Folder}\label{data-folder}}

This is a folder located in the main folder and is designed to hold data for this project.

\hypertarget{raw-folder}{%
\subsection{raw Folder}\label{raw-folder}}

This folder is located within the data folder and contains all raw data provided to the user. The purpose of the raw data is to simplify the data processing.

\begin{itemize}
\tightlist
\item
  \textbf{2007\_to\_2017\_NAICS.xlsx:} This file contains the cross walk used in the master script to update 2007 NAICs codes to 2017 NAICs codes. This file is a new development to automate error checking when linking IMPLAN codes to NAICs codes. It is NOT used in the Deprecated master script.
\item
  \textbf{2012\_2017\_NAICS\_to\_IMPLAN.xlsx:} This file contains the 2017 NAICS to IMPLAN crosswalk (available online from IMPLAN), as well as several additions. 2012 NAICS to IMPLAN values were appended to this file in addition to several 2002 NAICs codes that required updating. This file is a new development to automate error checking when linking IMPLAN codes to NAICs codes. It is NOT used in the Deprecated master script.
\item
  \textbf{business\_type\_to\_implan546\_crosswalk.csv:} This file was created to serve as a crosswalk between grant recipient business types and their corresponding IMPLAN codes. This file was made by manually looking up each business type and entering its corresponding IMPLAN code value.
\item
  \textbf{contract\_recipient\_to\_district\_crosswalk.xlxs:} This file was created to serve as an aid to error checking by assigning a District values to certain contract data that lacked them. Creation of this file involved looking up addresses of businesses with no given District in order to correctly assign a District to the data. It is NOT used in the Deprecated master script.
\end{itemize}

\hypertarget{blank_sheets_for_r-folder}{%
\subsubsection{Blank\_Sheets\_for\_R Folder}\label{blank_sheets_for_r-folder}}

This folder contains the template spreadsheets used in the final preparation of data for entry into IMPLAN.

\begin{itemize}
\tightlist
\item
  \textbf{CommodityOutput2.xlsx:}
\item
  \textbf{HouseholdSpendingChange4.xlsx:}
\item
  \textbf{IndustrySpendingPattern5.xlsx:}
\item
  \textbf{InstitutionSpendingPattern6.xlsx}
\item
  \textbf{LaborIncomeChange3.xlsx:}
\end{itemize}

\hypertarget{deprecated-folder}{%
\subsubsection{deprecated Folder}\label{deprecated-folder}}

This folder contains raw data only used by the Deprecated master script.

\begin{itemize}
\tightlist
\item
  \textbf{2017\_implan\_online\_naics\_to\_implan546.xlsx:} This file contains the crosswalk for linking 2017 IMPLAN codes to NAICs code.
\item
  \textbf{2021\_employment\_totals.xlsx:}
\item
  \textbf{DOD\_County\_Shares\_R.xlsx:} This file contains the percentage of each County by surface area that is assigned to each Californian Congressional District as of 2021.
\end{itemize}

\hypertarget{temp-folder}{%
\subsection{temp Folder}\label{temp-folder}}

This folder is also located within the data folder and holds all files that are made while either master script is running. These files are deleted when the final output is finished being processed.

\begin{itemize}
\tightlist
\item
  \textbf{placeholderfortemp.txt:} This file prevents Github from deleting the temp folder, as it is empty by default.
\end{itemize}

\hypertarget{output-folder}{%
\section{output Folder}\label{output-folder}}

This folder is within the main folder and holds all of the final output files needed by IMPLAN in order to complete this project. New output files will get created and placed into this folder as scripts are run.

\begin{itemize}
\tightlist
\item
  \textbf{placeholder.txt:} This file prevents Github from deleting the output folder, as it is empty by default.
\end{itemize}

\hypertarget{src-folder}{%
\section{src Folder}\label{src-folder}}

This folder within the main folder contains all script files needed by the master script when it is running. Scripts in this folder may be used by either of the master scripts contained in this repository. DO NOT MODIFY ANY OF THESE FILES.

\begin{itemize}
\tightlist
\item
  \textbf{aggregate\_usaspending.R:}
\item
  \textbf{concatenate\_usaspending.R:}
\item
  \textbf{error\_check\_and\_weight\_contracts.R:}
\item
  \textbf{error\_check\_grants.R:}
\item
  \textbf{filter\_usaspending.R:}
\item
  \textbf{obtain\_usaspending.R:}
\item
  \textbf{split\_usaspending.R:}
\end{itemize}

\hypertarget{deprecated-folder-1}{%
\subsection{deprecated Folder}\label{deprecated-folder-1}}

This folder within the src folder contains all scripts that are only used by the DEPRECATED\_run\_analysis\_master.R script. DO NOT MODIFY ANY OF THESE FILES.

\begin{itemize}
\tightlist
\item
  \textbf{DEPRECATED\_create\_implan\_sheets.R:}
\item
  \textbf{DEPRECATED\_error\_check\_contracts.R:}
\item
  \textbf{DEPRECATED\_error\_check\_grants.R:}
\end{itemize}

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

The following section details how to use the data and R code provided as well as an explanation of how the code works.

\hypertarget{reviewing-the-parameters-file}{%
\section{Reviewing the Parameters File}\label{reviewing-the-parameters-file}}

The first step in working with this code involves looking at the Parameters file. This file allows for alterations in the data analysis based on particular criteria that one may want to modify in their study, such as geography of interest, year, agencies of interest, and so forth. The default entries in the parameters file are based on this study of California.

\emph{Readme will eventually link to this section}

\hypertarget{making-user-specific-data-files}{%
\section{Making User Specific Data Files}\label{making-user-specific-data-files}}

The second step to focus on after modifying the parameters file is to ensure that user specific data files have been created in order to run this analysis. In this particular case, employment data (detailed in Section 3.1) and error check files were created in order to properly assign and allocate employment and spending data across California counties and congressional districts.

\hypertarget{processing-the-data}{%
\section{Processing the Data}\label{processing-the-data}}

After modifying the parameter files with the appropriate criteria and creating user specific data, one can begin working through the code to process the data and prepare it for IMPLAN. Refer to the \textbf{deprecated\_run\_analysis\_master.R} script to run through the data analysis process for this project.

\hypertarget{clearing-environment-and-loading-in-packages-and-parameters}{%
\subsection{Clearing Environment, and Loading in Packages and Parameters}\label{clearing-environment-and-loading-in-packages-and-parameters}}

The first lines of code provide some housekeeping steps prior to running through the analysis. Line 6 allows the user to clear their global environment in RStudio, removing all previously defined or loaded packages, variables, values, etc. In running through the analysis wholesale, a good first step is to clear one's environment. Following this is lines 9-14, which load in the various RStudio library packages that are needed to perform this analysis. For more information on these packages, please refer to our References section (``ADD IN LINK TO REFERENCES AND/OR R PACKAGE DESCRIPTIONS''). The final housekeeping step involves line 17, which loads in the parameters file mentioned above.

\hypertarget{loading-in-functions}{%
\subsection{Loading in Functions}\label{loading-in-functions}}

Lines 20-23 compose the second step of the data analysis process: loading in functions that standardize and simplify working through the data. Line 20 loads in a function that is used to place additional filters on the obtained USAspending data and write the filtered data into CSV files. These additional filters are also outlined in the parameters file, but the actual filter function is not run until lines 26-30 (see step 3 for more details).

Line 21 loads in a function that concatenates the USAspending contract and grant data files into one dataframe in R, while also removing direct payments from that concatenation (as it is calculated separate from contracts and grants. more on this will be detailed below). The actual concatenate function is not run until lines 39 and 41 (see step 5 for more details). Line 22 loads in a function that splits out the concatenated USAspending dataframe by agency, taking out DOE data from the main dataframe. This is done because DOE's spending activity has been kept separate from the main data analysis of the project. The actual split function is not run until lines 44-47 (see step 6 for more details). Line 23 loads in a function that aggregates the USAspending and DOEspending dataframes by IMPLAN sector. This gets the contracts and grants spending data for two IMPLAN activity sheets complete: one for the main statewide analysis, and one for the DOE statewide analysis. The actual aggregate function is not run until lines 55-60 (see step 7 for more details).

\hypertarget{obtaining-the-usaspending-data}{%
\subsection{Obtaining the USAspending Data}\label{obtaining-the-usaspending-data}}

Line 26 loads in a R script that performs a call to USAspending.gov's API that grabs the relevant contract, grant, and direct payment spending data that we need for our analysis and downloads it into two files (one for contracts, and one for grants/direct payments) at a specified file path location. In this case, the two CSV files obtained are downloaded into the data/temp folder. The parameters file is particularly important for this line of code, as the parameters file holds many of the filters that are used to sort what data is obtained from USAspending with this code.

\hypertarget{filtering-the-usaspending-data}{%
\subsection{Filtering the USAspending Data}\label{filtering-the-usaspending-data}}

Lines 29-34 begin the third step of filtering down the USAspending data obtained from the API call. Lines 29 and 30 read in the downloaded contracts and grants/direct payments files (respectively) and assign them each to a variable. Lines 33 and 34 perform the filter\_usaspending function (that was loaded in at line 20) on each of those variables.

The filter\_usaspending function has five elements that must be referenced: file\_name, state, doe\_filters, filters, and out\_name.

\begin{itemize}
\tightlist
\item
  The \textbf{file\_name} element is a call to whichever data file we want to pass through to be filtered - in this case, the two USAspending CSVs that are defined in lines 29 and 30.
\item
  The \textbf{state} element is defined in our parameters, based on the state of interest for our study. In this case, that state is California. This element is added to ensure that the primary\_place\_of\_performance\_state is the same as the recipient\_state (which was filtered in our obtain\_usaspending function), thus ensuring that all data entries included in the analysis are in the state of interest for the study.
\item
  The \textbf{doe\_filters} element is defined in our parameters, based on \textbf{doe\_offices}. This element is included to filter out DOE spending done in the state that is not national security-related. The basis for this filter element is from a list of DOE sub-agencies that were demarcated as national security-related.
\item
  The \textbf{filters} element is defined in our parameters, based on \textbf{contract\_columns} and \textbf{grant\_columns}, respectively. This element is included to pare down the columns of data from the USAspending files to retain only the pertinent columns of data for our analysis.
\item
  The \textbf{out\_name} element is defined in our parameters, based on \textbf{c\_out\_name} and \textbf{g\_out\_name}, respectively. This element is included to give a naming convention for the files that get generated as a result of running the filter function.
\end{itemize}

After running the filter functions, two CSVs files should be generated in the data/temp folder, by the name defined from the \textbf{out\_name} element.

\hypertarget{error-checking-the-usaspending-data}{%
\subsection{Error checking the USAspending Data}\label{error-checking-the-usaspending-data}}

Lines 37-38 begin the fourth step of checking for errors in the filtered USAspending data and fixing them to have an IMPLAN code associated with each entry. The process for error checking and fixing the data are relatively the same for contracts and grants - the only major difference being the variable of interest in fixing the errors. For contracts, the focus is on NAICS codes, while the grants look at business types.

\hypertarget{error-checking-contracts}{%
\subsubsection{Error Checking Contracts}\label{error-checking-contracts}}

Line 37 loads in a R script file that goes through the filtered USAspending contracts file and remediates issues in certain contract entries. The first lines in this R script load two files into two dataframes: 1) the filtered USAspending contracts file (\textbf{contracts}); and 2) a NAICS to IMPLAN crosswalk that was \href{https://support.implan.com/hc/en-us/articles/360034896614-546-Industries-Conversions-Bridges-Construction-2018-Data}{provided online on IMPLAN's website} (\textbf{naics\_to\_implan}). After reading in the NAICS to IMPLAN crosswalk, there are a couple of code fixes and removal of duplicate codes that our team determined to, in our best knowledge, be the most accurate way to relate those NAICS codes to an IMPLAN code. After this, the NAICS to IMPLAN crosswalk is merged into the contracts dataframe by NAICS code in order to get an IMPLAN code associated with each contract entry.

However, this merge does not give all contract entries an IMPLAN code. This can be for a variety of reasons, including: 1) a contract entry from USAspending not having a NAICS code; and 2) an incorrect and/or outdated NAICS code that was not a part of IMPLAN's provided crosswalk. The lines of code following the merge work to fix the error contract entries. First, the contracts with no NAICS codes were hardcoded in the \textbf{contracts} dataframe to a specific IMPLAN code based on the contract recipient's name and the industry which they work in. Next, the contracts which had a mistyped or older NAICS code were pulled out to a new dataframe (\textbf{contracts\_missing\_implan}) and hardcoded to an IMPLAN code based on our team's best research into which IMPLAN code each entry would best fit into. This presents a degree of error when parsing out the contract spending, but also represents the best available workaround given time and information constraints. A new way of handling the error checks for contracts is detailed in \href{LINK\%20TO\%20SECTION\%209\%20OF\%20PROCESS\%20GUIDE}{Section 9: What's Next}.

The final steps of this contracts error check involve dropping out the contract entries which had no IMPLAN code from the original \textbf{contracts} dataframe. Then, the dataframe which fixed the contracts that were missing IMPLAN codes (\textbf{contracts\_missing\_implan}) is merged back into the original \textbf{contracts} dataframe. The \textbf{contracts} dataframe now has all contract entries ``cleaned'' with an IMPLAN code for each entry. Last, only the columns necessary for this analysis are selected, and then the cleaned \textbf{contracts} dataframe is written into a new CSV file.

\hypertarget{error-checking-grantsdirect-payments}{%
\subsubsection{Error Checking Grants/Direct Payments}\label{error-checking-grantsdirect-payments}}

Line 38 loads in a R script file that goes through the filtered USAspending grants and direct payments file and remediates issues in certain grants entries. First, the script loads in the filtered USAspending grants and direct payments file into a dataframe (\textbf{grants}). Next, we pull out the VA direct payments from the \textbf{grants} dataframe, and define it into its' own dataframe (\textbf{va\_benefits}). This is done because VA direct payments does not a require an IMPLAN code and is thus calculated separate from contracts and grants from IMPLAN in the IMPLAN activity sheets. The \textbf{grants} dataframe filters out the direct payments based on the \textbf{assistance\_code} column of the data, where entries that have an assistance code of 10 are the direct payment entries.

After VA direct payments are separate from grants, the next step is to load a business type to IMPLAN crosswalk into a dataframe (\textbf{business\_to\_implan}). This file was self-created by the team as the best method for relating grants data to IMPLAN codes, given that USAspending grants data entries do not provide for NAICS codes. The business type to crosswalk is then merged with the \textbf{grants} dataframe in order to get an IMPLAN code associated with every grant entry.

However, this merge does not give all grant entries an IMPLAN code. In the case of the grants data, they may be missing a business type value, or their business type may be one that is not captured in the self-created crosswalk. Thus, the lines after the merge work to hardcode IMPLAN codes into the \textbf{grants} dataframe based on the grant recipient's name and the industry which they work in. After these hardcodes, a second dataframe (\textbf{grants\_missing\_implan}) is defined to capture if any other grant entries are missing an IMPLAN code. If a grant entry is still missing an IMPLAN code, it would need to be fixed in the \textbf{grants\_missing\_implan} dataframe and then merged back into the original \textbf{grants} dataframe.

Now that the grants data has been ``cleaned'', the final steps in this script involve selecting only the necessary columns of data for the \textbf{va\_benefits} and \textbf{grants} dataframes, and writing each one into their own respective CSV files.

\hypertarget{manually-fixing-congressional-district-errors}{%
\subsection{Manually Fixing Congressional District Errors}\label{manually-fixing-congressional-district-errors}}

Lines 40 and 42 provide an important notice to only continue with the code once some manual fixes have been implemented in the three cleaned CSV files produced from the above error check codes. \textbf{This step is essential to ensuring no weird errors come out along the way.} The issue in the USAspending data for contracts, grants, and direct payments is the \textbf{recipient\_congressional\_district} column, where certain entries in all three CSV files have a district value of ``NA'' (contracts) or ``90'' (grants, and direct payments). There may be multiple reasons for this error, with the most plausible being that these data entries are in locations that span across multiple counties and/or congressional districts.

In order to properly remediate these issues, our team utilized the \emph{DOD\_County\_Shares\_R.xlsx} file in the raw data folder. Looking at this file, the ``Districts'' tab in that Excel sheet provides a breakdown of how much a county spans across one or multiple congressional district(s) based on land area from the California Redistricting Commission. We would then filter the contract, grant, or direct payment entries by county, and randomly assign a certain number of entries to a district based on their county. For example, if 20 contract entries that had a ``NA'' value in the district column were in Alameda County, we would assign 47\% of those entries (\textasciitilde9) to CA-13, 42\% (\textasciitilde8) to CA-15, and 12\% (\textasciitilde3) to CA-17.

This method also inherently presents a degree of error with allocating spending across congressional districts. Even so, given time constraints, this workaround presented itself as the best solution at the time. Our team is working on a new way to address these manual fixes through code - for more details, refer to \href{LINK\%20TO\%20SECTION\%209\%20OF\%20PROCESS\%20GUIDE}{Section 9: What's Next}.

\hypertarget{concatenating-the-usaspending-data}{%
\subsection{Concatenating the USAspending Data}\label{concatenating-the-usaspending-data}}

Lines 45-46

\hypertarget{splitting-doe-from-usaspending-data}{%
\subsection{Splitting DOE from USAspending Data}\label{splitting-doe-from-usaspending-data}}

Lines 49-52

\hypertarget{aggregating-the-spending-data-for-statewide-numbers}{%
\subsection{Aggregating the spending Data for Statewide Numbers}\label{aggregating-the-spending-data-for-statewide-numbers}}

Lines 58-59 and 61-63

\hypertarget{compiling-employment-data}{%
\subsection{Compiling Employment Data}\label{compiling-employment-data}}

Lines ??-??

\hypertarget{running-for-loops-to-generate-implan-activity-sheets}{%
\subsection{Running For Loops to Generate IMPLAN Activity Sheets}\label{running-for-loops-to-generate-implan-activity-sheets}}

Line ??

\begin{itemize}
\item
  Process data

  \begin{itemize}
  \tightlist
  \item
    Clean contracts and grant data-
  \item
    Clean spending data
  \item
    Error check contract spending data
  \end{itemize}

  Will need to go into detail about changes in the code between this year (2021) and subsequent years

  More detailed mention of how the error checking of the USASpending.gov contract data is needed
  A detailed walk through of how to manually check data and use the multiple NAICS to IMPLAN crosswalks to catch data
  Mention how IMPLAN automatically removes any codes having to do with construction so those have to be manually coded

  Some errors occur due to the transaction not being given a NAICS code, those need to be manually fixed

  Issues occur with NAICS codes that apply to multiple IMPLAN codes- give detailed explanation of how this was worked around and data was processed and added back to the main cleaned data.
\end{itemize}

\hypertarget{using-implan}{%
\chapter{Using IMPLAN}\label{using-implan}}

Once the IMPLAN activity sheets for your desired geographies have been produced, it is time to upload them into IMPLAN and allow the tool to run its analysis. When you log into IMPLAN, the homepage should display 3 boxes to navigate through: Regions, Impacts, and Projects.

\begin{itemize}
\item
  \textbf{Regions:} Click on Regions to begin your analysis. This should open up a map of the United States. Select the region of interest for the analysis (state, county, congressional district, etc.) and give this project a name. After saving the project, move on to the Impacts tab.
\item
  \textbf{Impacts:} In the Impacts tab, look for the button to the right of the ``Save'' button. Click on that, and select ``upload event template''. From there, find the activity sheet for the geography of interest and select that for upload. Ensure that ``Industry Change'' and ``Household Spending'' can be selected, and choose those for uploading into IMPLAN. The data points for the respective IMPLAN sectors should then auto-populate on IMPLAN. If your analysis is including SmartPay spending data, be sure to manually add that in as type ``Institutional Spending'', specification 11001, and the value amount. Choose to select all events (the button to the left of the ``Save'' button) and drag them over to the geography's model on the right side of the IMPLAN screen. After that, select to run the model at the bottom right. Depending on how big the model, IMPLAN will take time to generate the results. Once the model is done with its analysis, click on the ``View Results'' button.
\item
  \textbf{Results:} Once on this page, one can sort through the results of the IMPLAN activity sheet that was ran for a geography and download CSV files of a variety of data points. For this report, the data points of interest included economic indicators like output and full-time employment (FTE) opportunities, both generally and across industries, as well as government and tax revenue generated.
\end{itemize}

As part of our final step before developing and writing the reports, our team created code to combine all the input and output data from IMPLAN into one big spreadsheet which was used to create the graphs and charts seen throughout the reports.

\hypertarget{conclusion-discussion}{%
\chapter{Conclusion/ Discussion}\label{conclusion-discussion}}

Hopefully we provide some guidelines and aid in discovering and processing this data so that quality studies can come about and
Government spending can become more transparent.

Utilizing IMPLAN to manage government spending data doesn't come with a lot of instructions

You have to trust that the data entry is accurate, many instances of older NAICS codes exist and this has the potential to introduce errors into the code.

Limited time to process the data due to a new method of processing the data

\hypertarget{whats-next}{%
\chapter{What's Next?}\label{whats-next}}

A section on where we hope to add and develop this process. Potentially the section to outline changes to the code we have already made for the upcoming 2021 report.

\hypertarget{license-your-gitbook}{%
\chapter{License your GitBook}\label{license-your-gitbook}}

In the spirit of Open Science, it is good to think about making your course materials Open Source. That means that other people can use them. In principle, if you publish materials online without license information, you hold the copyright to those materials. If you want them to be Open Source, you must include a license. It is not always obvious what license to choose.

The Creative Commons licenses are typically suitable for course materials. This GitBook, for example, is licensed under CC-BY 4.0. That means you can use and remix it as you like, but you must credit the original source.

If your project is more focused on software or source code, consider using the \href{https://www.gnu.org/licenses/gpl-3.0.en.html}{GNU GPL v3 license} instead.

You can find \href{https://creativecommons.org/share-your-work/licensing-examples}{more information about the Creative Commons Licenses here}. Specific licenses that might be useful are:

\begin{itemize}
\tightlist
\item
  \href{https://creativecommons.org/share-your-work/public-domain/cc0/}{CC0 (``No Rights Reserved'')}, everybody can do what they want with your work.
\item
  \href{https://creativecommons.org/licenses/by/4.0/}{CC-BY 4.0 (``Attribution'')}, everybody can do what they want with your work, but they must credit you. Note that this license may not be suitable for software or source code!
\end{itemize}

For compatibility between CC and GNU licenses, see \href{https://creativecommons.org/faq/\#Can_I_apply_a_Creative_Commons_license_to_software.3F}{this FAQ}.

  \bibliography{literature.bib}

\end{document}
